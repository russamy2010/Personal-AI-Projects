{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkJFPXP9bF0q"
      },
      "source": [
        "**Clicked Advanced Machine Learning: Multi-linear Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sB56KeObEVo"
      },
      "outputs": [],
      "source": [
        "#import libraries--may not use all of them\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "import patsy\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import plot_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbZ6X7YhfoxP"
      },
      "source": [
        "**Import Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCfPWDw6foWv"
      },
      "outputs": [],
      "source": [
        "data = \"/content/ClickedDemostration_PredictiveAnalytics.csv\"\n",
        "#column_names = ['Car_Make', 'Year', 'Selling_Price (in 000s)', 'Present_Price  (in 000s)', 'KM_Driven',\n",
        "               # 'Fuel_Type', 'Fuel_Type_Category', 'Seller_Type', 'Seller_Type_Category', 'Transmission'#\n",
        "                #'Transmission_Category','Owner','Owner_Category']#\n",
        "\n",
        "raw_dataset = pd.read_csv(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SzMUL_4g6pd"
      },
      "outputs": [],
      "source": [
        "dataset = raw_dataset.copy()\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB3J1I7Qg0o8"
      },
      "source": [
        "**Check for NA Values and Drop if Need Be**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKu5ny9XhXaY"
      },
      "outputs": [],
      "source": [
        "dataset.isna().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJXxm23QhbO0"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nfsGfwbhibm"
      },
      "source": [
        "**One hot encoding**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtzGBHV-i0oO"
      },
      "outputs": [],
      "source": [
        "#Create category maps--done separately since each is a different dictionary\n",
        "dataset['Seller_Type_Category'] = dataset['Seller_Type_Category'].map({1: 'Dealer', 2: 'Individual'})\n",
        "dataset['Transmission_Category'] = dataset['Transmission_Category'].map({1: 'Manual', 2: 'Automatic'})\n",
        "dataset['Fuel_Type_Category'] = dataset['Fuel_Type_Category'].map({1: 'Petrol', 2: 'Diesel', 3: 'CNG'})\n",
        "dataset['Owner_Category'] = dataset['Owner_Category'].map({0: 'New', 1: 'One', 2: 'Two', 3: 'Three'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmeCbaqkjnj8"
      },
      "outputs": [],
      "source": [
        "#One hot encode variables\n",
        "dataset = pd.get_dummies(dataset, columns=['Seller_Type_Category', 'Transmission_Category',\n",
        "                                           'Fuel_Type_Category', 'Owner_Category'], prefix='', prefix_sep='')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Training and Test Dataset**"
      ],
      "metadata": {
        "id": "VZeZD24rql6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=dataset.copy\n",
        "train_dataset = train_dataset()\n",
        "test_dataset=dataset.copy\n",
        "test_dataset = test_dataset()"
      ],
      "metadata": {
        "id": "XPDkWr-Yqlcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C38EmUuinO7Z"
      },
      "source": [
        "**Inspect data**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_Xnhcrr1aDC"
      },
      "outputs": [],
      "source": [
        "#Check for non-numerical information and convert to categorical data\n",
        "check_non_numerical_columns=[train_dataset, test_dataset]\n",
        "\n",
        "def check_non_numerical_columns(datasets):\n",
        "  for dataset in datasets:\n",
        "    for column in dataset.columns:\n",
        "      if train_dataset[column].dtype == 'object':\n",
        "        print(f\"Non-numerical data found in column: {column}\")\n",
        "        print(train_dataset[column].unique())# Added and will test\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4OhNKFh2bIk"
      },
      "outputs": [],
      "source": [
        "#Update columns to categorical\n",
        "train_dataset['Car_Make'] = train_dataset['Car_Make'].astype('category')\n",
        "test_dataset['Car_Make'] = test_dataset['Car_Make'].astype('category')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Car_Make and ANOVA**"
      ],
      "metadata": {
        "id": "IuDGPHVyXO_N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfzec1Ph8KgZ"
      },
      "outputs": [],
      "source": [
        "# Create train_anova dataset to ensure data integrity\n",
        "def train_anova(train_dataset):\n",
        "  anova_data = train_dataset[['Car_Make', 'Selling_Price (in 000s)']].copy()\n",
        "  anova_data.rename(columns={'Selling_Price (in 000s)': 'Selling_Price_in_000s'}, inplace=True)\n",
        "  return anova_data\n",
        "\n",
        "  anova_data = train_anova(train_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit an ANOVA model to determine if strong relation btw Car_Make and Price\n",
        "anova_data = train_anova(train_dataset)\n",
        "anova_model = ols('Selling_Price_in_000s ~ C(Car_Make)', data=anova_data).fit()\n",
        "anova_table = sm.stats.anova_lm(anova_model, typ=1)\n",
        "\n",
        "# Print the ANOVA table\n",
        "print(anova_table)"
      ],
      "metadata": {
        "id": "Y9IorK-Jtxph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAEP0KswG-u5"
      },
      "outputs": [],
      "source": [
        "#Drop Car_Make from datasets\n",
        "train_dataset = train_dataset.drop(columns=['Car_Make'])\n",
        "test_dataset = test_dataset.drop(columns=['Car_Make'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build Decision Tree and Logistical Regression**"
      ],
      "metadata": {
        "id": "SGF7LH-TOQk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepocess data\n",
        "X = train_dataset.drop('Seller_Type',axis=1)\n",
        "y = train_dataset['Seller_Type']\n",
        "\n",
        "#Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "vVMykfaQO2bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create and Train Tree\n",
        "clf = DecisionTreeClassifier(max_depth=3)  # Customize parameters later (e.g., max_depth)\n",
        "clf = clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "py5IGwmnTJyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualize results\n",
        "plt.figure(figsize=(20,10))  # Adjust figsize if needed\n",
        "tree.plot_tree(clf)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b-IBMNqdS5_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature importance\n",
        "feature_importances = clf.feature_importances_\n",
        "most_important_features = np.argsort(feature_importances)[::-1]  # Indices of most important features first\n",
        "\n",
        "# Example: Print top features\n",
        "for i in range(5):\n",
        "    print(X.columns[most_important_features[i]], feature_importances[most_important_features[i]])"
      ],
      "metadata": {
        "id": "PsoAHvyYS-RK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistical Model**"
      ],
      "metadata": {
        "id": "0Mk3j8euTCtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Logistic Model\n",
        "logistic_model = LogisticRegression()\n",
        "logistic_model.fit(X_train, y_train)\n",
        "\n",
        "# Predicted probabilities\n",
        "y_pred_proba = logistic_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
        "\n",
        "# Predictions (using a threshold of 0.5)\n",
        "y_pred = (y_pred_proba >= 0.5).astype(int)  # Convert probabilities to binary predictions"
      ],
      "metadata": {
        "id": "yqDIg1JRO-Gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Coefficients and intercepts\n",
        "coefficients = logistic_model.coef_[0]  # Assuming binary classification\n",
        "intercept = logistic_model.intercept_[0]\n",
        "\n",
        "# Get features\n",
        "feature_names = X_train.columns\n",
        "\n",
        "# Construct the Equation using Logistic Sigmoid (log_odds)\n",
        "equation = f\"Log-odds(Target = 1) = {intercept:.2f} \"\n",
        "for i, coeff in enumerate(coefficients):\n",
        "    sign = \"+\" if coeff >= 0 else \"-\"\n",
        "    equation += f\" {sign} {abs(coeff):.2f} * {feature_names[i]} \"\n",
        "\n",
        "def logistic_sigmoid(log_odds):\n",
        "    return 1 / (1 + np.exp(-log_odds))\n",
        "#Calculate the log-odds for all data points\n",
        "log_odds = intercept + np.dot(X_test.values, coefficients)\n",
        "\n",
        "# Calculate probabilities\n",
        "probabilities = logistic_sigmoid(log_odds)\n",
        "\n",
        "print(probabilities)"
      ],
      "metadata": {
        "id": "OM7a8fQT1C2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\\n\", confusion_mat)\n",
        "print(\"ROC-AUC Score:\", roc_auc)"
      ],
      "metadata": {
        "id": "3LnZhf0dTjvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**START OF MULTI-LINEAR SECTION--Additional Data Review: Joint Distribution and Multi-collinearity**"
      ],
      "metadata": {
        "id": "BB4CdwO0XUTR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXnqV90YnTjS"
      },
      "outputs": [],
      "source": [
        "#Review joint distribution\n",
        "sns.pairplot(train_dataset[['Selling_Price (in 000s)', 'KM_Driven', 'Fuel_Type', 'Year', 'Seller_Type',\n",
        "                            'Transmission','Owner']], diag_kind='kde')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czgcjZCv7U6m"
      },
      "outputs": [],
      "source": [
        "#Check for multi-collinearity\n",
        "train_dataset.describe().transpose()\n",
        "def plot_correlation_matrix(train_dataset,exclude_columns=['Selling_Price (in 000s)']):\n",
        "  corr_matrix = train_dataset.drop(exclude_columns, axis=1).corr()\n",
        "  sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
        "  plt.show()\n",
        "\n",
        "plot_correlation_matrix(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u68Lkx1Dlj3Q"
      },
      "source": [
        "**Split data into test, validation, and training sets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8F46teJjlwY7"
      },
      "outputs": [],
      "source": [
        "# Split into training and testing sets--80/20\n",
        "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
        "test_dataset = dataset.drop(train_dataset.index)\n",
        "\n",
        "#Drop Car_Make\n",
        "train_dataset = train_dataset.drop(columns=['Car_Make'])\n",
        "test_dataset = test_dataset.drop(columns=['Car_Make'])\n",
        "'''\n",
        "Split the training set again into training and validation sets (e.g., 70/30 split)\n",
        "train_dataset, val_dataset = train_dataset.sample(frac=0.7, random_state=0).reset_index(drop=True),\n",
        "train_dataset.drop(train_dataset.index).reset_index(drop=True)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3tQ09Dsz8ne"
      },
      "outputs": [],
      "source": [
        "#Review the training and test set\n",
        "train_dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKiLX0ceo5o8"
      },
      "source": [
        "**Split features from labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shvoX7czo25D"
      },
      "outputs": [],
      "source": [
        "train_features = train_dataset.copy()\n",
        "test_features = test_dataset.copy()\n",
        "\n",
        "train_labels = train_features.pop('Selling_Price (in 000s)')\n",
        "test_labels = test_features.pop('Selling_Price (in 000s)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktrp7e0JpxDK"
      },
      "source": [
        "**Normalize the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myWwhhpnp1Ze"
      },
      "outputs": [],
      "source": [
        "#Create normalization layer\n",
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "\n",
        "#Normalize the data by fitting to the preprocessing layer\n",
        "normalizer.adapt(np.array(train_features))\n",
        "\n",
        "#Calculate the mean and variance, and store them in the layer:\n",
        "print(normalizer.mean.numpy())\n",
        "\n",
        "#When called, will produce input data with all features independently normalized\n",
        "first = np.array(train_features[:1])\n",
        "\n",
        "with np.printoptions(precision=2, suppress=True):\n",
        "  print('First example:', first)\n",
        "  print()\n",
        "  print('Normalized:', normalizer(first).numpy())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDOw21V_qk0g"
      },
      "source": [
        "**Build Multi-Linear Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es-_XqEGNPW6"
      },
      "outputs": [],
      "source": [
        "#Review shape of all printouts\n",
        "print(\"train_features shape:\", train_features.shape)\n",
        "print(\"train_labels shape:\", train_labels.shape)\n",
        "print(\"test_features shape:\", test_features.shape)\n",
        "print(\"test_labels shape:\", test_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS7aYsxHrOBm"
      },
      "outputs": [],
      "source": [
        "#First, build and train multi-linear model\n",
        "linear_model = tf.keras.Sequential([\n",
        "    normalizer,\n",
        "    layers.Dense(units=1),\n",
        "    #layers.Input(shape=(7,))#\n",
        "])\n",
        "\n",
        "#Call model.predict\n",
        "linear_model.predict(train_features[:9])\n",
        "\n",
        "#Confirm kernal weights--should be (8,1)\n",
        "linear_model.layers[1].kernel\n",
        "\n",
        "#Compile the model\n",
        "linear_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error')\n",
        "\n",
        "#%%time#\n",
        "history = linear_model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    epochs=50,\n",
        "    # Suppress logging.\n",
        "    verbose=0,\n",
        "    # Calculate validation results on 20% of the training data.\n",
        "    validation_split = 0.2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ48413ltlAE"
      },
      "outputs": [],
      "source": [
        "#Plot loss history\n",
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  plt.ylim([0, 10])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Error')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "\n",
        "plot_loss(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIN6G0xwtqKL"
      },
      "outputs": [],
      "source": [
        "#Collect Results\n",
        "\n",
        "#print(plot_model(linear_model))\n",
        "test_results = {}\n",
        "test_results['linear_model'] = linear_model.evaluate(\n",
        "    test_features, test_labels, verbose=0)\n",
        "\n",
        "print(test_results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Show linear_model equation\n",
        "\n",
        "#Get weights (coefficients)\n",
        "coefficients = linear_model.layers[1].kernel.numpy().flatten()\n",
        "#print(coefficients)\n",
        "\n",
        "\n",
        "#Get bias (intercept)\n",
        "intercept = linear_model.layers[1].bias.numpy()\n",
        "#print(intercept)\n",
        "\n",
        "#Get number of features\n",
        "num_features = train_features.shape[1]\n",
        "#print(num_features)\n",
        "\n",
        "#Construct Equation\n",
        "def model_equation(coefficients, intercept, train_features, num_features):\n",
        "  equation = f\"Predicted_Value = {intercept[0]:.2f} \"  # Adjust formatting as needed\n",
        "  for i in range(num_features):\n",
        "    sign = \"+\" if coefficients[i] >= 0 else \"-\"\n",
        "    equation += f\" {sign} {abs(coefficients[i]):.2f} * {train_features.columns[i]} \"\n",
        "  #print(\"Equation:\", equation)\n",
        "  return equation\n",
        "\n",
        "result = model_equation(coefficients, intercept, train_features, num_features)\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "iN_olxshGI2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3_XfcxMK78K"
      },
      "outputs": [],
      "source": [
        "#Check features and label shape\n",
        "print(f\"test_features shape: {test_features.shape}\")\n",
        "print(f\"test_labels shape: {test_labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cbi_Y9TAuB09"
      },
      "source": [
        "**BONUS: MultiLinear Regression with Deep Neural Network--MUST RUN MULTI-LINEAR SECTION FIRST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zRDPkDguFuq"
      },
      "outputs": [],
      "source": [
        "def build_and_compile_model(norm):\n",
        "  model = keras.Sequential([\n",
        "      norm,\n",
        "      layers.Dense(64, activation='relu'),\n",
        "      layers.Dense(64, activation='relu'),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  model.compile(loss='mean_absolute_error',\n",
        "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOAAxe2OuTnU"
      },
      "outputs": [],
      "source": [
        "dnn_model = build_and_compile_model(normalizer)\n",
        "dnn_model.summary()\n",
        "\n",
        "##%%time\n",
        "history = dnn_model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    validation_split=0.2,\n",
        "    verbose=0, epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WubfpQjuaeb"
      },
      "outputs": [],
      "source": [
        "#Plot loss history\n",
        "plot_loss(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvJsmun4uf5b"
      },
      "outputs": [],
      "source": [
        "#Collect results\n",
        "test_results['dnn_model'] = dnn_model.evaluate(test_features, test_labels, verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM4tvh0bundq"
      },
      "source": [
        "**Performance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvYLBQ7cunIs"
      },
      "outputs": [],
      "source": [
        "#Review Linear vs. DNN Model\n",
        "pd.DataFrame(test_results,\n",
        "             index=['Mean absolute error [Selling_Price (in 000s)]']).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzDMraN8u_h9"
      },
      "source": [
        "**Make Predictions and review the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1a2o8QWvE4w"
      },
      "outputs": [],
      "source": [
        "test_predictions = dnn_model.predict(test_features).flatten()\n",
        "\n",
        "a = plt.axes(aspect='equal')\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True Values [Purchase_Price (in 000s)]')\n",
        "plt.ylabel('Predictions [Selling_Price (in 000s]')\n",
        "lims = [0, 50]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "_ = plt.plot(lims, lims)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OdkLR3XvPUY"
      },
      "outputs": [],
      "source": [
        "#View error terms\n",
        "error = test_predictions - test_labels\n",
        "plt.hist(error, bins=30)\n",
        "plt.xlabel('Selling_Price (in 000s)')\n",
        "_ = plt.ylabel('Count')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VBosuSYvVJJ"
      },
      "outputs": [],
      "source": [
        "#Save model\n",
        "dnn_model.save('dnn_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5P1StbAPvgDH"
      },
      "outputs": [],
      "source": [
        "#Review results\n",
        "pd.DataFrame(test_results, index=['Selling_Price (in 000s)']).T"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPa9vTLP1FTKxM7LAP39alS"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}